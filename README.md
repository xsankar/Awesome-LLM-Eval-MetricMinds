### Awesome-LLM-Eval-MetricMinds : All things for LLM Evaluation
#### _[Back to TOC](https://github.com/xsankar/Awesome-Awesome-LLM)_
***
| [Survey Paper Collection](#survey-papers) | [General Theory](#general-theory) | [Evaluation Experiments](#evaluation-experiments) | [Multi Benchmark Frameworks](#multi-benchmark-frameworks) |[Frameworks & Benchmarks by topic](#frameworks--benchmarks-by-topic) | [Datasets](#datasets) | [Other Repos](#other-repos) |
| :-: | :-: | :-: | :-: |:-: |:-: |:-: |
***
### Some Concepts
![OpenAI](./images/NPS-v07-p26.png)
***
![OpenAI](./images/NPS-v07-p27.png)
***
![OpenAI](./images/NPS-v07-p29.png)
***
![OpenAI](./images/NPS-v07-p30.png)
***
![OpenAI](./images/NPS-v07-p31.png)
***
| [Survey Paper Collection](#survey-papers) | [General Theory](#general-theory) | [Evaluation Experiments](#evaluation-experiments) | [Multi Benchmark Frameworks](#multi-benchmark-frameworks) |[Frameworks & Benchmarks by topic](#frameworks--benchmarks-by-topic) | [Datasets](#datasets) | [Other Repos](#other-repos) |
| :-: | :-: | :-: | :-: |:-: |:-: |:-: |
***
## Survey Papers [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Year | Title | Notes | 
| -: | :- | :- |
| 10.2023 | [Cataloguing LLM Evaluations](https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf) | from [AI verify Foundation SIngapore](https://aiverifyfoundation.sg/) |
| 10.2023 | [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736) | |
| 7.2023 | [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109) | |
| 11.2022 | [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) |  |
***
## General Theory [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Year | Title | Notes | 
| :-: | :-: | :-: |
|  |  | 
***
## Evaluation Experiments [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Year | Title | Notes | 
| -: | :- | :- |
| 11.2021 | [A Systematic Investigation of Commonsense Knowledge in Large Language Models](https://arxiv.org/abs/2111.00607) | EMNLP/Dubai 2022 | 
***
## Multi Benchmark Frameworks [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Year | Title | Notes | 
| :- | :- | :- |
| 2023 | [MosaicML Eval Gauntlet](https://www.mosaicml.com/llm-evaluation) | Good eval framework. Encompasses 34 different benchmarks, organized into 6 broad categories of competency that we expect good foundation models to have. |
| 2023 | [OpenCompass](https://github.com/open-compass/opencompass) | Extensive dataset support, good dataset list |
***
## Frameworks & Benchmarks by topic [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
---
| [World Knowledge](#world-knowledge) | [Commonsense Reasoning](#commonsense-reasoning) | [Language Understanding](#language-understanding) | [Symbolic Problem Solving](#symbolic-problem-solving) |[Math QA](#math-qa) | [Reading Comprehension](#reading-comprehension) | 
| :-: | :-: | :-: | :-: |:-: |:-: |
***
## World Knowledge [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
---
## Commonsense Reasoning [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
---
## Language Understanding [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
---
## Symbolic Problem Solving [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
---
## Math QA [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
---
## Reading Comprehension [<img src="images/back_button.png" width="25" height="25">](#frameworks--benchmarks-by-topic)
| Year | Title | Notes | 
| :- | :- | :- |
|  |  |  |
***
## Datasets [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Year | Title | Notes | 
| :- | :- | :- |
|  | [Jeopardy Data at Higginface](https://huggingface.co/datasets/jeopardy) |  |
***
## Other Repos [<img src="images/back_button_2.png" width="25" height="25">](#some-concepts)
| Repo | Notes | 
| :-: | :-: |
|[Papers and resources for LLMs evaluation](https://github.com/MLGroupJLU/LLM-eval-survey) | Exhaustive list of benchmarks |

 
